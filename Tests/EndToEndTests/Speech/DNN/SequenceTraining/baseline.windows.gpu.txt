CPU info:
    CPU Model Name: Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz
    Hardware threads: 6
    Total Memory: 58719796 kB
-------------------------------------------------------------------
=== Running /cygdrive/c/jenkins/workspace/CNTK-Test-Windows-W1/x64/release/cntk.exe configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/cntk_sequence.cntk currentDirectory=C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData RunDir=C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu DataDir=C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining OutputDir=C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu DeviceId=0 timestamping=true
CNTK 2.3.1+ (HEAD df860b, Jan 11 2018 19:57:21) at 2018/01/12 01:37:30

C:\jenkins\workspace\CNTK-Test-Windows-W1\x64\release\cntk.exe  configFile=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/cntk_sequence.cntk  currentDirectory=C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData  RunDir=C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu  DataDir=C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData  ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining  OutputDir=C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu  DeviceId=0  timestamping=true
Changed current directory to C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData
-------------------------------------------------------------------
Build info: 

		Built time: Jan 11 2018 19:47:28
		Last modified date: Thu Jan 11 19:34:23 2018
		Build type: Release
		Build target: GPU
		With 1bit-SGD: no
		With ASGD: yes
		Math lib: mkl
		CUDA version: 9.0.0
		CUDNN version: 7.0.5
		Build Branch: HEAD
		Build SHA1: df860b6311a807a8b1b6fea7c378e491da47250a
		MPI distribution: Microsoft MPI
		MPI version: 7.0.12437.6
-------------------------------------------------------------------
-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 3072; computeCapability = 5.2; type = "Tesla M60"; total memory = 8124 MB; free memory = 8001 MB
-------------------------------------------------------------------

Configuration After Processing and Variable Resolution:

configparameters: cntk_sequence.cntk:addLayer2=[    
    action = "edit"
    currLayer = 1
    newLayer = 2
    currModel = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    newModel  = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]

configparameters: cntk_sequence.cntk:AddLayer3=[    
    action = "edit"
    currLayer = 2
    newLayer = 3
    currModel = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    newModel  = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/add_layer.mel"
]

configparameters: cntk_sequence.cntk:command=dptPre1:addLayer2:dptPre2:addLayer3:speechTrain:replaceCriterionNode:sequenceTrain
configparameters: cntk_sequence.cntk:ConfigDir=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining
configparameters: cntk_sequence.cntk:currentDirectory=C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData
configparameters: cntk_sequence.cntk:DataDir=C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData
configparameters: cntk_sequence.cntk:deviceId=0
configparameters: cntk_sequence.cntk:dptPre1=[
    action = "train"
    modelPath = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:dptPre2=[
    action = "train"
    modelPath = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech"
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn_1layer.txt"
    ]
]

configparameters: cntk_sequence.cntk:globalInvStdPath=GlobalStats/var.363
configparameters: cntk_sequence.cntk:globalMeanPath=GlobalStats/mean.363
configparameters: cntk_sequence.cntk:globalPriorPath=GlobalStats/prior.132
configparameters: cntk_sequence.cntk:ndlMacros=C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/macros.txt
configparameters: cntk_sequence.cntk:OutputDir=C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu
configparameters: cntk_sequence.cntk:precision=float
configparameters: cntk_sequence.cntk:reader=[
    readerType = "HTKMLFReader"
    readMethod = "blockRandomize"
    miniBatchMode = "partial"
    randomize = "auto"
    verbosity = 0
    features = [
        dim = 363
        type = "real"
        scpFile = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
    ]
    labels = [
        mlfFile = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
        labelMappingFile = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
        labelDim = 132
        labelType = "category"
    ]
]

configparameters: cntk_sequence.cntk:replaceCriterionNode=[
    action = "edit"
    currModel = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    newModel  = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0"
    editPath  = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/replace_ce_with_sequence_criterion.mel"
]

configparameters: cntk_sequence.cntk:RunDir=C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu
configparameters: cntk_sequence.cntk:sequenceTrain=[
    action = "train"
    modelPath = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/nonexistentfile.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 10
        learningRatesPerSample = 0.000002
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 3
        hsmoothingWeight = 0.95
        frameDropThresh = 1e-10
        numMBsToShowResult = 10
        gradientClippingWithTruncation = true
        clippingThresholdPerSample = 1.0
    ]
    reader = [
        readerType = "HTKMLFReader"
        readMethod = "blockRandomize"
        frameMode = false
        nbruttsineachrecurrentiter = 2
        miniBatchMode = "partial"
        randomize = "auto"
        verbosity = 0
        features = [
            dim = 363
            type = "real"
            scpFile = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp"
        ]
        labels = [
            mlfFile = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf"
            labelMappingFile = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list"
            labelDim = 132
            labelType = "category"
        ]
        hmms = [
            phoneFile  = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/model.overalltying"
            transpFile = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/model.transprob"
        ]
        lattices = [
            denlatTocFile = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/*.lats.toc"
        ]
    ]
]

configparameters: cntk_sequence.cntk:SGD=[
    epochSize = 81920
    minibatchSize = 256
    learningRatesPerMB = 0.8
    numMBsToShowResult = 10
    momentumPerMB = 0.9
    dropoutRate = 0.0
    maxEpochs = 2
]

configparameters: cntk_sequence.cntk:speechTrain=[
    action = "train"
    modelPath = "C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech"
    traceLevel = 1
    NDLNetworkBuilder = [
        networkDescription = "C:\jenkins\workspace\CNTK-Test-Windows-W1\Tests\EndToEndTests\Speech\DNN\SequenceTraining/dnn.txt"
    ]
    SGD = [
        epochSize = 81920
        minibatchSize = 256:512
        learningRatesPerMB = 0.8:1.6
        numMBsToShowResult = 10
        momentumPerSample = 0.999589
        dropoutRate = 0.0
        maxEpochs = 4
        gradUpdateType = "none"
        normWithAveMultiplier = true
        clippingThresholdPerSample = 1#INF
    ]
]

configparameters: cntk_sequence.cntk:timestamping=true
configparameters: cntk_sequence.cntk:traceLevel=1
configparameters: cntk_sequence.cntk:truncated=false
01/12/2018 01:37:30: Commands: dptPre1 addLayer2 dptPre2 addLayer3 speechTrain replaceCriterionNode sequenceTrain
01/12/2018 01:37:30: precision = "float"

01/12/2018 01:37:30: ##############################################################################
01/12/2018 01:37:30: #                                                                            #
01/12/2018 01:37:30: # dptPre1 command (train action)                                             #
01/12/2018 01:37:30: #                                                                            #
01/12/2018 01:37:30: ##############################################################################

01/12/2018 01:37:30: 
Creating virgin network.
NDLBuilder Using GPU 0
Microsoft::MSR::CNTK::GPUMatrix<ElemType>::SetUniformRandomValue (GPU): creating curand object with seed 1, sizeof(ElemType)==4
reading script file C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
01/12/2018 01:37:31: 
Model has 19 nodes. Using GPU 0.

01/12/2018 01:37:31: Training criterion:   ce = CrossEntropyWithSoftmax
01/12/2018 01:37:31: Evaluation criterion: err = ClassificationError


Allocating matrices for forward and/or backward propagation.

Gradient Memory Aliasing: 2 are aliased.
	OL.t (gradient) reuses OL.z (gradient)

Memory Sharing: Out of 29 matrices, 11 are shared as 3, and 18 are not shared.

Here are the ones that share memory:
	{ HL1.W : [512 x 363] (gradient)
	  HL1.t : [512 x *]
	  HL1.y : [512 x 1 x *] }
	{ HL1.t : [512 x *] (gradient)
	  HL1.y : [512 x 1 x *] (gradient)
	  HL1.z : [512 x 1 x *]
	  OL.z : [132 x 1 x *] }
	{ HL1.z : [512 x 1 x *] (gradient)
	  OL.t : [132 x 1 x *]
	  OL.t : [132 x 1 x *] (gradient)
	  OL.z : [132 x 1 x *] (gradient) }

Here are the ones that don't share memory:
	{scaledLogLikelihood : [132 x 1 x *]}
	{features : [363 x *]}
	{globalMean : [363 x 1]}
	{labels : [132 x *]}
	{globalInvStd : [363 x 1]}
	{globalPrior : [132 x 1]}
	{HL1.b : [512 x 1]}
	{OL.b : [132 x 1]}
	{OL.W : [132 x 512]}
	{HL1.W : [512 x 363]}
	{err : [1]}
	{ce : [1]}
	{featNorm : [363 x *]}
	{OL.W : [132 x 512] (gradient)}
	{HL1.b : [512 x 1] (gradient)}
	{logPrior : [132 x 1]}
	{OL.b : [132 x 1] (gradient)}
	{ce : [1] (gradient)}


01/12/2018 01:37:31: Training 254084 parameters in 4 out of 4 parameter tensors and 10 nodes with gradient:

01/12/2018 01:37:31: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
01/12/2018 01:37:31: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
01/12/2018 01:37:31: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
01/12/2018 01:37:31: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

01/12/2018 01:37:31: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

01/12/2018 01:37:31: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

01/12/2018 01:37:31: Starting minibatch loop.
01/12/2018 01:37:31:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 3.89978256 * 2560; err = 0.84375000 * 2560; time = 0.2249s; samplesPerSecond = 11384.4
01/12/2018 01:37:31:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.96755676 * 2560; err = 0.72031250 * 2560; time = 0.0105s; samplesPerSecond = 244489.4
01/12/2018 01:37:31:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.55723495 * 2560; err = 0.65859375 * 2560; time = 0.0105s; samplesPerSecond = 244868.3
01/12/2018 01:37:31:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 2.29642715 * 2560; err = 0.61992187 * 2560; time = 0.0104s; samplesPerSecond = 245203.7
01/12/2018 01:37:31:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 2.02396469 * 2560; err = 0.55117187 * 2560; time = 0.0106s; samplesPerSecond = 242314.1
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.87309265 * 2560; err = 0.51484375 * 2560; time = 0.0106s; samplesPerSecond = 241170.4
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.78157349 * 2560; err = 0.50507813 * 2560; time = 0.0105s; samplesPerSecond = 244938.6
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.75391235 * 2560; err = 0.50781250 * 2560; time = 0.0104s; samplesPerSecond = 246030.8
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.66460266 * 2560; err = 0.45742187 * 2560; time = 0.0104s; samplesPerSecond = 245091.0
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.62184143 * 2560; err = 0.47968750 * 2560; time = 0.0105s; samplesPerSecond = 244086.2
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.65327911 * 2560; err = 0.47265625 * 2560; time = 0.0104s; samplesPerSecond = 245521.2
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.50686646 * 2560; err = 0.44921875 * 2560; time = 0.0104s; samplesPerSecond = 245766.3
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.46723328 * 2560; err = 0.42304687 * 2560; time = 0.0104s; samplesPerSecond = 245267.1
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.49163208 * 2560; err = 0.44140625 * 2560; time = 0.0104s; samplesPerSecond = 245617.7
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.46437378 * 2560; err = 0.43398437 * 2560; time = 0.0105s; samplesPerSecond = 243668.0
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.43047485 * 2560; err = 0.43867187 * 2560; time = 0.0111s; samplesPerSecond = 229965.6
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.42106323 * 2560; err = 0.41953125 * 2560; time = 0.0131s; samplesPerSecond = 196167.1
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.46542969 * 2560; err = 0.42421875 * 2560; time = 0.0110s; samplesPerSecond = 233098.1
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.47425842 * 2560; err = 0.44062500 * 2560; time = 0.0099s; samplesPerSecond = 259592.8
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.42849121 * 2560; err = 0.44062500 * 2560; time = 0.0097s; samplesPerSecond = 264315.4
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.34078064 * 2560; err = 0.41210938 * 2560; time = 0.0097s; samplesPerSecond = 263339.3
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.39478760 * 2560; err = 0.42734375 * 2560; time = 0.0098s; samplesPerSecond = 262203.7
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.40148315 * 2560; err = 0.41210938 * 2560; time = 0.0097s; samplesPerSecond = 262939.0
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.39321899 * 2560; err = 0.42617187 * 2560; time = 0.0098s; samplesPerSecond = 260025.2
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.32427368 * 2560; err = 0.40195313 * 2560; time = 0.0097s; samplesPerSecond = 263713.6
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.27012024 * 2560; err = 0.39921875 * 2560; time = 0.0097s; samplesPerSecond = 263645.7
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.32388916 * 2560; err = 0.39218750 * 2560; time = 0.0097s; samplesPerSecond = 263819.6
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.25467834 * 2560; err = 0.38359375 * 2560; time = 0.0097s; samplesPerSecond = 263887.6
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.23462830 * 2560; err = 0.37226562 * 2560; time = 0.0097s; samplesPerSecond = 264209.0
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.20836792 * 2560; err = 0.35937500 * 2560; time = 0.0097s; samplesPerSecond = 263556.2
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.23704834 * 2560; err = 0.36796875 * 2560; time = 0.0097s; samplesPerSecond = 263564.3
01/12/2018 01:37:32:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.23081055 * 2560; err = 0.37539062 * 2560; time = 0.0099s; samplesPerSecond = 259224.8
01/12/2018 01:37:32: Finished Epoch[ 1 of 2]: [Training] ce = 1.65178680 * 81920; err = 0.46788330 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.682703s
01/12/2018 01:37:32: SGD: Saving checkpoint model 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech.1'

01/12/2018 01:37:32: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

01/12/2018 01:37:32: Starting minibatch loop.
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.21959934 * 2560; err = 0.37109375 * 2560; time = 0.0114s; samplesPerSecond = 225054.9
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.18487740 * 2560; err = 0.36601563 * 2560; time = 0.0090s; samplesPerSecond = 283386.5
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.17382298 * 2560; err = 0.35937500 * 2560; time = 0.0090s; samplesPerSecond = 284925.7
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.20050812 * 2560; err = 0.35820313 * 2560; time = 0.0092s; samplesPerSecond = 277269.3
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.19367180 * 2560; err = 0.37929687 * 2560; time = 0.0090s; samplesPerSecond = 284741.8
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.16213341 * 2560; err = 0.34335938 * 2560; time = 0.0090s; samplesPerSecond = 285701.5
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.13570023 * 2560; err = 0.35078125 * 2560; time = 0.0095s; samplesPerSecond = 269193.2
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.19333878 * 2560; err = 0.37148437 * 2560; time = 0.0090s; samplesPerSecond = 285615.5
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.23995056 * 2560; err = 0.37773438 * 2560; time = 0.0092s; samplesPerSecond = 277925.5
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.18914948 * 2560; err = 0.36562500 * 2560; time = 0.0090s; samplesPerSecond = 285341.7
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.16878815 * 2560; err = 0.35820313 * 2560; time = 0.0090s; samplesPerSecond = 284574.1
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.24551544 * 2560; err = 0.38242188 * 2560; time = 0.0090s; samplesPerSecond = 283920.8
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.18659210 * 2560; err = 0.35039063 * 2560; time = 0.0090s; samplesPerSecond = 283933.4
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.21731110 * 2560; err = 0.37031250 * 2560; time = 0.0142s; samplesPerSecond = 180733.5
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.19863586 * 2560; err = 0.37109375 * 2560; time = 0.0187s; samplesPerSecond = 136543.5
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.14327850 * 2560; err = 0.34453125 * 2560; time = 0.0096s; samplesPerSecond = 266638.9
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.14432526 * 2560; err = 0.35742188 * 2560; time = 0.0087s; samplesPerSecond = 295373.3
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.17652893 * 2560; err = 0.35195312 * 2560; time = 0.0086s; samplesPerSecond = 298800.1
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.15350037 * 2560; err = 0.35898438 * 2560; time = 0.0090s; samplesPerSecond = 283625.1
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.08181000 * 2560; err = 0.33320312 * 2560; time = 0.0085s; samplesPerSecond = 301126.9
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.14218140 * 2560; err = 0.34960938 * 2560; time = 0.0086s; samplesPerSecond = 298469.2
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.17052460 * 2560; err = 0.35898438 * 2560; time = 0.0085s; samplesPerSecond = 301268.6
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.19296570 * 2560; err = 0.37929687 * 2560; time = 0.0084s; samplesPerSecond = 303890.0
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.15556946 * 2560; err = 0.35078125 * 2560; time = 0.0084s; samplesPerSecond = 305132.4
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.15362244 * 2560; err = 0.35351563 * 2560; time = 0.0087s; samplesPerSecond = 293379.5
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.08062744 * 2560; err = 0.32617188 * 2560; time = 0.0084s; samplesPerSecond = 305176.1
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.09230347 * 2560; err = 0.34570313 * 2560; time = 0.0088s; samplesPerSecond = 291177.1
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.06243591 * 2560; err = 0.33671875 * 2560; time = 0.0087s; samplesPerSecond = 293891.4
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.09511108 * 2560; err = 0.33281250 * 2560; time = 0.0084s; samplesPerSecond = 305449.2
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.14536438 * 2560; err = 0.35273437 * 2560; time = 0.0087s; samplesPerSecond = 295060.0
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.10204773 * 2560; err = 0.34218750 * 2560; time = 0.0087s; samplesPerSecond = 293645.3
01/12/2018 01:37:32:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.07106323 * 2560; err = 0.32734375 * 2560; time = 0.0121s; samplesPerSecond = 211322.2
01/12/2018 01:37:32: Finished Epoch[ 2 of 2]: [Training] ce = 1.15852671 * 81920; err = 0.35554199 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.307949s
01/12/2018 01:37:32: SGD: Saving checkpoint model 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/Pre1/cntkSpeech'

01/12/2018 01:37:32: Action "train" complete.


01/12/2018 01:37:32: ##############################################################################
01/12/2018 01:37:32: #                                                                            #
01/12/2018 01:37:32: # addLayer2 command (edit action)                                            #
01/12/2018 01:37:32: #                                                                            #
01/12/2018 01:37:32: ##############################################################################


01/12/2018 01:37:32: Action "edit" complete.


01/12/2018 01:37:32: ##############################################################################
01/12/2018 01:37:32: #                                                                            #
01/12/2018 01:37:32: # dptPre2 command (train action)                                             #
01/12/2018 01:37:32: #                                                                            #
01/12/2018 01:37:32: ##############################################################################

01/12/2018 01:37:32: 
Starting from checkpoint. Loading network from 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.0'.
NDLBuilder Using GPU 0
reading script file C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
01/12/2018 01:37:32: 
Model has 24 nodes. Using GPU 0.

01/12/2018 01:37:32: Training criterion:   ce = CrossEntropyWithSoftmax
01/12/2018 01:37:32: Evaluation criterion: err = ClassificationError

01/12/2018 01:37:32: Training 516740 parameters in 6 out of 6 parameter tensors and 15 nodes with gradient:

01/12/2018 01:37:32: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
01/12/2018 01:37:32: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
01/12/2018 01:37:32: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
01/12/2018 01:37:32: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
01/12/2018 01:37:32: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
01/12/2018 01:37:32: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

01/12/2018 01:37:32: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

01/12/2018 01:37:32: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

01/12/2018 01:37:33: Starting minibatch loop.
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[   1-  10, 3.13%]: ce = 4.76453552 * 2560; err = 0.80664063 * 2560; time = 0.0195s; samplesPerSecond = 131003.2
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[  11-  20, 6.25%]: ce = 2.85073395 * 2560; err = 0.69375000 * 2560; time = 0.0111s; samplesPerSecond = 230056.5
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[  21-  30, 9.38%]: ce = 2.28368835 * 2560; err = 0.59257812 * 2560; time = 0.0113s; samplesPerSecond = 227099.3
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.94338684 * 2560; err = 0.52148438 * 2560; time = 0.0111s; samplesPerSecond = 230435.5
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.72908173 * 2560; err = 0.48085937 * 2560; time = 0.0111s; samplesPerSecond = 230622.3
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.62283478 * 2560; err = 0.46640625 * 2560; time = 0.0111s; samplesPerSecond = 229887.1
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.51846161 * 2560; err = 0.44609375 * 2560; time = 0.0112s; samplesPerSecond = 227703.3
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.50159607 * 2560; err = 0.44218750 * 2560; time = 0.0111s; samplesPerSecond = 230568.3
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.46150055 * 2560; err = 0.42265625 * 2560; time = 0.0111s; samplesPerSecond = 230643.1
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.41712341 * 2560; err = 0.41406250 * 2560; time = 0.0111s; samplesPerSecond = 231287.0
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.42065277 * 2560; err = 0.41250000 * 2560; time = 0.0120s; samplesPerSecond = 213125.5
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.34741516 * 2560; err = 0.39765625 * 2560; time = 0.0113s; samplesPerSecond = 225670.0
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.32274933 * 2560; err = 0.39140625 * 2560; time = 0.0111s; samplesPerSecond = 229839.7
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.33492279 * 2560; err = 0.39296875 * 2560; time = 0.0120s; samplesPerSecond = 213203.6
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.32771606 * 2560; err = 0.39140625 * 2560; time = 0.0174s; samplesPerSecond = 146852.1
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.28157349 * 2560; err = 0.39101562 * 2560; time = 0.0155s; samplesPerSecond = 164968.6
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.29435425 * 2560; err = 0.38242188 * 2560; time = 0.0136s; samplesPerSecond = 187745.2
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.29888611 * 2560; err = 0.38945313 * 2560; time = 0.0167s; samplesPerSecond = 153217.3
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.33448181 * 2560; err = 0.40429688 * 2560; time = 0.0112s; samplesPerSecond = 228759.3
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.32330017 * 2560; err = 0.40898438 * 2560; time = 0.0104s; samplesPerSecond = 245081.6
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.23590393 * 2560; err = 0.37382813 * 2560; time = 0.0104s; samplesPerSecond = 246288.8
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.27341003 * 2560; err = 0.38867188 * 2560; time = 0.0105s; samplesPerSecond = 244400.7
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.26567078 * 2560; err = 0.37460938 * 2560; time = 0.0121s; samplesPerSecond = 211794.3
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.24478149 * 2560; err = 0.36914063 * 2560; time = 0.0104s; samplesPerSecond = 245787.5
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.21717224 * 2560; err = 0.36757812 * 2560; time = 0.0104s; samplesPerSecond = 245483.5
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.18839111 * 2560; err = 0.36835937 * 2560; time = 0.0117s; samplesPerSecond = 218726.8
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.23902588 * 2560; err = 0.36601563 * 2560; time = 0.0104s; samplesPerSecond = 245712.0
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.18707581 * 2560; err = 0.36093750 * 2560; time = 0.0108s; samplesPerSecond = 236513.6
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.16996765 * 2560; err = 0.35195312 * 2560; time = 0.0133s; samplesPerSecond = 192707.2
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.14782104 * 2560; err = 0.34179688 * 2560; time = 0.0175s; samplesPerSecond = 146035.4
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.17535706 * 2560; err = 0.35234375 * 2560; time = 0.0139s; samplesPerSecond = 184829.5
01/12/2018 01:37:33:  Epoch[ 1 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.19136047 * 2560; err = 0.37031250 * 2560; time = 0.0136s; samplesPerSecond = 188507.0
01/12/2018 01:37:33: Finished Epoch[ 1 of 2]: [Training] ce = 1.52859163 * 81920; err = 0.42607422 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.535292s
01/12/2018 01:37:33: SGD: Saving checkpoint model 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech.1'

01/12/2018 01:37:33: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.900000  momentum as time constant = 2429.8 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

01/12/2018 01:37:33: Starting minibatch loop.
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[   1-  10, 3.13%]: ce = 1.18167496 * 2560; err = 0.35195312 * 2560; time = 0.0126s; samplesPerSecond = 202736.9
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[  11-  20, 6.25%]: ce = 1.15440292 * 2560; err = 0.35664062 * 2560; time = 0.0099s; samplesPerSecond = 259153.9
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[  21-  30, 9.38%]: ce = 1.15731888 * 2560; err = 0.34804687 * 2560; time = 0.0108s; samplesPerSecond = 237439.4
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[  31-  40, 12.50%]: ce = 1.13935738 * 2560; err = 0.34335938 * 2560; time = 0.0118s; samplesPerSecond = 216455.7
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[  41-  50, 15.63%]: ce = 1.14808121 * 2560; err = 0.36367187 * 2560; time = 0.0175s; samplesPerSecond = 146618.3
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[  51-  60, 18.75%]: ce = 1.14439125 * 2560; err = 0.33867188 * 2560; time = 0.0159s; samplesPerSecond = 160873.7
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[  61-  70, 21.88%]: ce = 1.09884491 * 2560; err = 0.34179688 * 2560; time = 0.0161s; samplesPerSecond = 159470.8
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[  71-  80, 25.00%]: ce = 1.15980606 * 2560; err = 0.35859375 * 2560; time = 0.0149s; samplesPerSecond = 171624.3
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[  81-  90, 28.13%]: ce = 1.17044067 * 2560; err = 0.36093750 * 2560; time = 0.0095s; samplesPerSecond = 270315.9
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[  91- 100, 31.25%]: ce = 1.11957779 * 2560; err = 0.34531250 * 2560; time = 0.0097s; samplesPerSecond = 265024.1
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 101- 110, 34.38%]: ce = 1.11841125 * 2560; err = 0.34570313 * 2560; time = 0.0093s; samplesPerSecond = 274866.9
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 111- 120, 37.50%]: ce = 1.19153061 * 2560; err = 0.36328125 * 2560; time = 0.0092s; samplesPerSecond = 276858.5
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 121- 130, 40.63%]: ce = 1.13423920 * 2560; err = 0.33945313 * 2560; time = 0.0093s; samplesPerSecond = 275541.4
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 131- 140, 43.75%]: ce = 1.16283112 * 2560; err = 0.35312500 * 2560; time = 0.0092s; samplesPerSecond = 278554.6
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 141- 150, 46.88%]: ce = 1.12025299 * 2560; err = 0.34335938 * 2560; time = 0.0092s; samplesPerSecond = 277732.6
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 151- 160, 50.00%]: ce = 1.09458618 * 2560; err = 0.33750000 * 2560; time = 0.0097s; samplesPerSecond = 264974.7
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 161- 170, 53.13%]: ce = 1.10369873 * 2560; err = 0.33984375 * 2560; time = 0.0092s; samplesPerSecond = 278094.6
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 171- 180, 56.25%]: ce = 1.11500549 * 2560; err = 0.32890625 * 2560; time = 0.0092s; samplesPerSecond = 277026.3
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 181- 190, 59.38%]: ce = 1.09892426 * 2560; err = 0.33554688 * 2560; time = 0.0092s; samplesPerSecond = 278475.8
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 191- 200, 62.50%]: ce = 1.06063843 * 2560; err = 0.33007813 * 2560; time = 0.0094s; samplesPerSecond = 273177.4
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 201- 210, 65.63%]: ce = 1.10568542 * 2560; err = 0.34257813 * 2560; time = 0.0094s; samplesPerSecond = 272184.1
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 211- 220, 68.75%]: ce = 1.13217773 * 2560; err = 0.34414062 * 2560; time = 0.0092s; samplesPerSecond = 278136.9
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 221- 230, 71.88%]: ce = 1.12288818 * 2560; err = 0.33984375 * 2560; time = 0.0092s; samplesPerSecond = 278149.0
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 231- 240, 75.00%]: ce = 1.10664063 * 2560; err = 0.33789063 * 2560; time = 0.0092s; samplesPerSecond = 278191.3
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 241- 250, 78.13%]: ce = 1.09506836 * 2560; err = 0.33281250 * 2560; time = 0.0092s; samplesPerSecond = 277943.7
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 251- 260, 81.25%]: ce = 1.04699707 * 2560; err = 0.32109375 * 2560; time = 0.0093s; samplesPerSecond = 276088.2
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 261- 270, 84.38%]: ce = 1.04719543 * 2560; err = 0.33632812 * 2560; time = 0.0092s; samplesPerSecond = 278206.4
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 271- 280, 87.50%]: ce = 1.03957214 * 2560; err = 0.32382813 * 2560; time = 0.0092s; samplesPerSecond = 278251.8
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 281- 290, 90.63%]: ce = 1.05847168 * 2560; err = 0.32695313 * 2560; time = 0.0093s; samplesPerSecond = 274442.5
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 291- 300, 93.75%]: ce = 1.11241150 * 2560; err = 0.33867188 * 2560; time = 0.0092s; samplesPerSecond = 278218.5
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 301- 310, 96.88%]: ce = 1.08333740 * 2560; err = 0.33398438 * 2560; time = 0.0092s; samplesPerSecond = 278427.3
01/12/2018 01:37:33:  Epoch[ 2 of 2]-Minibatch[ 311- 320, 100.00%]: ce = 1.04971313 * 2560; err = 0.32226563 * 2560; time = 0.0094s; samplesPerSecond = 273087.1
01/12/2018 01:37:33: Finished Epoch[ 2 of 2]: [Training] ce = 1.11481791 * 81920; err = 0.34144287 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.336659s
01/12/2018 01:37:33: SGD: Saving checkpoint model 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/Pre2/cntkSpeech'

01/12/2018 01:37:34: Action "train" complete.


01/12/2018 01:37:34: ##############################################################################
01/12/2018 01:37:34: #                                                                            #
01/12/2018 01:37:34: # addLayer3 command (edit action)                                            #
01/12/2018 01:37:34: #                                                                            #
01/12/2018 01:37:34: ##############################################################################


01/12/2018 01:37:34: Action "edit" complete.


01/12/2018 01:37:34: ##############################################################################
01/12/2018 01:37:34: #                                                                            #
01/12/2018 01:37:34: # speechTrain command (train action)                                         #
01/12/2018 01:37:34: #                                                                            #
01/12/2018 01:37:34: ##############################################################################

01/12/2018 01:37:34: 
Starting from checkpoint. Loading network from 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.0'.
NDLBuilder Using GPU 0
reading script file C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
total 132 state names in state list C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
...............................................................................................feature set 0: 252734 frames in 948 out of 948 utterances
label set 0: 129 classes
minibatchutterancesource: 948 utterances grouped into 3 chunks, av. chunk size: 316.0 utterances, 84244.7 frames
01/12/2018 01:37:34: 
Model has 29 nodes. Using GPU 0.

01/12/2018 01:37:34: Training criterion:   ce = CrossEntropyWithSoftmax
01/12/2018 01:37:34: Evaluation criterion: err = ClassificationError

01/12/2018 01:37:34: Training 779396 parameters in 8 out of 8 parameter tensors and 20 nodes with gradient:

01/12/2018 01:37:34: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
01/12/2018 01:37:34: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
01/12/2018 01:37:34: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
01/12/2018 01:37:34: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
01/12/2018 01:37:34: 	Node 'HL3.W' (LearnableParameter operation) : [512 x 512]
01/12/2018 01:37:34: 	Node 'HL3.b' (LearnableParameter operation) : [512 x 1]
01/12/2018 01:37:34: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
01/12/2018 01:37:34: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

01/12/2018 01:37:34: No PreCompute nodes found, or all already computed. Skipping pre-computation step.

01/12/2018 01:37:34: Starting Epoch 1: learning rate per sample = 0.003125  effective momentum = 0.900117  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

01/12/2018 01:37:34: Starting minibatch loop.
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[   1-  10, 3.13%]: ce = 4.10357437 * 2560; err = 0.82031250 * 2560; time = 0.0208s; samplesPerSecond = 122932.1
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[  11-  20, 6.25%]: ce = 2.57244034 * 2560; err = 0.63671875 * 2560; time = 0.0119s; samplesPerSecond = 214784.9
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[  21-  30, 9.38%]: ce = 2.03364334 * 2560; err = 0.54179687 * 2560; time = 0.0118s; samplesPerSecond = 216576.6
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[  31-  40, 12.50%]: ce = 1.73527985 * 2560; err = 0.47500000 * 2560; time = 0.0118s; samplesPerSecond = 216829.7
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[  41-  50, 15.63%]: ce = 1.54782410 * 2560; err = 0.43945313 * 2560; time = 0.0118s; samplesPerSecond = 216761.8
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[  51-  60, 18.75%]: ce = 1.44337387 * 2560; err = 0.41210938 * 2560; time = 0.0118s; samplesPerSecond = 216772.8
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[  61-  70, 21.88%]: ce = 1.36145630 * 2560; err = 0.40585938 * 2560; time = 0.0203s; samplesPerSecond = 126341.8
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[  71-  80, 25.00%]: ce = 1.35890045 * 2560; err = 0.39804688 * 2560; time = 0.0119s; samplesPerSecond = 215194.8
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[  81-  90, 28.13%]: ce = 1.34205933 * 2560; err = 0.38945313 * 2560; time = 0.0121s; samplesPerSecond = 210888.8
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[  91- 100, 31.25%]: ce = 1.30447235 * 2560; err = 0.38046875 * 2560; time = 0.0155s; samplesPerSecond = 164968.6
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 101- 110, 34.38%]: ce = 1.31331329 * 2560; err = 0.38710937 * 2560; time = 0.0119s; samplesPerSecond = 214947.2
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 111- 120, 37.50%]: ce = 1.23968048 * 2560; err = 0.36953125 * 2560; time = 0.0119s; samplesPerSecond = 215439.3
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 121- 130, 40.63%]: ce = 1.21697845 * 2560; err = 0.35742188 * 2560; time = 0.0121s; samplesPerSecond = 211997.7
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 131- 140, 43.75%]: ce = 1.24119873 * 2560; err = 0.36992188 * 2560; time = 0.0120s; samplesPerSecond = 213072.3
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 141- 150, 46.88%]: ce = 1.23539429 * 2560; err = 0.37382813 * 2560; time = 0.0119s; samplesPerSecond = 215837.0
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 151- 160, 50.00%]: ce = 1.19547272 * 2560; err = 0.35195312 * 2560; time = 0.0120s; samplesPerSecond = 213418.7
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 161- 170, 53.13%]: ce = 1.21609497 * 2560; err = 0.36250000 * 2560; time = 0.0120s; samplesPerSecond = 213830.5
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 171- 180, 56.25%]: ce = 1.24842529 * 2560; err = 0.37500000 * 2560; time = 0.0120s; samplesPerSecond = 212638.7
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 181- 190, 59.38%]: ce = 1.26809998 * 2560; err = 0.38359375 * 2560; time = 0.0119s; samplesPerSecond = 214425.1
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 191- 200, 62.50%]: ce = 1.22535706 * 2560; err = 0.38632813 * 2560; time = 0.0126s; samplesPerSecond = 203484.7
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 201- 210, 65.63%]: ce = 1.17337036 * 2560; err = 0.35781250 * 2560; time = 0.0119s; samplesPerSecond = 214871.5
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 211- 220, 68.75%]: ce = 1.19753418 * 2560; err = 0.36679688 * 2560; time = 0.0121s; samplesPerSecond = 211428.7
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 221- 230, 71.88%]: ce = 1.21829834 * 2560; err = 0.36015625 * 2560; time = 0.0119s; samplesPerSecond = 215999.1
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 231- 240, 75.00%]: ce = 1.18372192 * 2560; err = 0.35390625 * 2560; time = 0.0118s; samplesPerSecond = 216245.4
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 241- 250, 78.13%]: ce = 1.16905823 * 2560; err = 0.35781250 * 2560; time = 0.0118s; samplesPerSecond = 216146.8
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 251- 260, 81.25%]: ce = 1.12310791 * 2560; err = 0.35000000 * 2560; time = 0.0120s; samplesPerSecond = 213477.4
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 261- 270, 84.38%]: ce = 1.18959351 * 2560; err = 0.35781250 * 2560; time = 0.0118s; samplesPerSecond = 216198.0
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 271- 280, 87.50%]: ce = 1.13449707 * 2560; err = 0.35273437 * 2560; time = 0.0124s; samplesPerSecond = 207170.0
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 281- 290, 90.63%]: ce = 1.12417908 * 2560; err = 0.33476563 * 2560; time = 0.0118s; samplesPerSecond = 216739.8
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 291- 300, 93.75%]: ce = 1.10653687 * 2560; err = 0.33867188 * 2560; time = 0.0121s; samplesPerSecond = 211733.0
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 301- 310, 96.88%]: ce = 1.12954407 * 2560; err = 0.34414062 * 2560; time = 0.0118s; samplesPerSecond = 216475.8
01/12/2018 01:37:34:  Epoch[ 1 of 4]-Minibatch[ 311- 320, 100.00%]: ce = 1.12625122 * 2560; err = 0.34765625 * 2560; time = 0.0118s; samplesPerSecond = 216631.5
01/12/2018 01:37:34: Finished Epoch[ 1 of 4]: [Training] ce = 1.40871038 * 81920; err = 0.40120850 * 81920; totalSamplesSeen = 81920; learningRatePerSample = 0.003125; epochTime=0.540956s
01/12/2018 01:37:34: SGD: Saving checkpoint model 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.1'

01/12/2018 01:37:35: Starting Epoch 2: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81920), data subset 0 of 1, with 1 datapasses

01/12/2018 01:37:35: Starting minibatch loop.
01/12/2018 01:37:35:  Epoch[ 2 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.22988195 * 5120; err = 0.37324219 * 5120; time = 0.0257s; samplesPerSecond = 199142.8
01/12/2018 01:37:35:  Epoch[ 2 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.17050295 * 5120; err = 0.34707031 * 5120; time = 0.0176s; samplesPerSecond = 290965.3
01/12/2018 01:37:35:  Epoch[ 2 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.17057648 * 5120; err = 0.36054687 * 5120; time = 0.0175s; samplesPerSecond = 292998.4
01/12/2018 01:37:35:  Epoch[ 2 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.29325256 * 5120; err = 0.38945313 * 5120; time = 0.0175s; samplesPerSecond = 292434.4
01/12/2018 01:37:35:  Epoch[ 2 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.15675087 * 5120; err = 0.36054687 * 5120; time = 0.0192s; samplesPerSecond = 266295.0
01/12/2018 01:37:35:  Epoch[ 2 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.13633652 * 5120; err = 0.34531250 * 5120; time = 0.0325s; samplesPerSecond = 157303.2
01/12/2018 01:37:35:  Epoch[ 2 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.12745209 * 5120; err = 0.34179688 * 5120; time = 0.0212s; samplesPerSecond = 241832.3
01/12/2018 01:37:35:  Epoch[ 2 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.11147766 * 5120; err = 0.34628906 * 5120; time = 0.0174s; samplesPerSecond = 294112.6
01/12/2018 01:37:35:  Epoch[ 2 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.09499435 * 5120; err = 0.33671875 * 5120; time = 0.0174s; samplesPerSecond = 294528.8
01/12/2018 01:37:35:  Epoch[ 2 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.07211533 * 5120; err = 0.32812500 * 5120; time = 0.0174s; samplesPerSecond = 294099.1
01/12/2018 01:37:35:  Epoch[ 2 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.12221298 * 5120; err = 0.34238281 * 5120; time = 0.0174s; samplesPerSecond = 294762.8
01/12/2018 01:37:35:  Epoch[ 2 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.11653061 * 5120; err = 0.34785156 * 5120; time = 0.0174s; samplesPerSecond = 294694.9
01/12/2018 01:37:35:  Epoch[ 2 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.05828247 * 5120; err = 0.32636719 * 5120; time = 0.0173s; samplesPerSecond = 295485.8
01/12/2018 01:37:35:  Epoch[ 2 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.03556671 * 5120; err = 0.32656250 * 5120; time = 0.0174s; samplesPerSecond = 294771.3
01/12/2018 01:37:35:  Epoch[ 2 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.09171906 * 5120; err = 0.33105469 * 5120; time = 0.0175s; samplesPerSecond = 292593.2
01/12/2018 01:37:35:  Epoch[ 2 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.06842804 * 5120; err = 0.32578125 * 5120; time = 0.0178s; samplesPerSecond = 287343.4
01/12/2018 01:37:35: Finished Epoch[ 2 of 4]: [Training] ce = 1.12850504 * 81920; err = 0.34556885 * 81920; totalSamplesSeen = 163840; learningRatePerSample = 0.003125; epochTime=0.312349s
01/12/2018 01:37:35: SGD: Saving checkpoint model 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.2'

01/12/2018 01:37:35: Starting Epoch 3: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 163840), data subset 0 of 1, with 1 datapasses

01/12/2018 01:37:35: Starting minibatch loop.
01/12/2018 01:37:35:  Epoch[ 3 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.13445969 * 5120; err = 0.34375000 * 5120; time = 0.0188s; samplesPerSecond = 271926.7
01/12/2018 01:37:35:  Epoch[ 3 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.07075939 * 5120; err = 0.33300781 * 5120; time = 0.0174s; samplesPerSecond = 293480.4
01/12/2018 01:37:35:  Epoch[ 3 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.06535053 * 5120; err = 0.33085938 * 5120; time = 0.0174s; samplesPerSecond = 294043.3
01/12/2018 01:37:35:  Epoch[ 3 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.07702370 * 5120; err = 0.33242187 * 5120; time = 0.0175s; samplesPerSecond = 292409.4
01/12/2018 01:37:35:  Epoch[ 3 of 4]-Minibatch[  41-  50, 31.25%]: ce = 1.07252197 * 5120; err = 0.32792969 * 5120; time = 0.0174s; samplesPerSecond = 293658.8
01/12/2018 01:37:35:  Epoch[ 3 of 4]-Minibatch[  51-  60, 37.50%]: ce = 1.05518150 * 5120; err = 0.32910156 * 5120; time = 0.0175s; samplesPerSecond = 291844.3
01/12/2018 01:37:35:  Epoch[ 3 of 4]-Minibatch[  61-  70, 43.75%]: ce = 1.06431198 * 5120; err = 0.32734375 * 5120; time = 0.0174s; samplesPerSecond = 293505.6
01/12/2018 01:37:35:  Epoch[ 3 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.07353668 * 5120; err = 0.32578125 * 5120; time = 0.0175s; samplesPerSecond = 293292.1
01/12/2018 01:37:35:  Epoch[ 3 of 4]-Minibatch[  81-  90, 56.25%]: ce = 1.03901520 * 5120; err = 0.31894531 * 5120; time = 0.0177s; samplesPerSecond = 289384.9
01/12/2018 01:37:35:  Epoch[ 3 of 4]-Minibatch[  91- 100, 62.50%]: ce = 1.05586166 * 5120; err = 0.32753906 * 5120; time = 0.0174s; samplesPerSecond = 294097.4
01/12/2018 01:37:35:  Epoch[ 3 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 1.04341202 * 5120; err = 0.32363281 * 5120; time = 0.0174s; samplesPerSecond = 294034.9
01/12/2018 01:37:35:  Epoch[ 3 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.07875061 * 5120; err = 0.33691406 * 5120; time = 0.0176s; samplesPerSecond = 290098.1
01/12/2018 01:37:35:  Epoch[ 3 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.10670013 * 5120; err = 0.33203125 * 5120; time = 0.0174s; samplesPerSecond = 293957.2
01/12/2018 01:37:35:  Epoch[ 3 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 1.06287842 * 5120; err = 0.32656250 * 5120; time = 0.0174s; samplesPerSecond = 294131.2
01/12/2018 01:37:35:  Epoch[ 3 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 1.05311127 * 5120; err = 0.33398438 * 5120; time = 0.0174s; samplesPerSecond = 294208.9
01/12/2018 01:37:35:  Epoch[ 3 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 1.04087524 * 5120; err = 0.32753906 * 5120; time = 0.0175s; samplesPerSecond = 291887.6
01/12/2018 01:37:35: Finished Epoch[ 3 of 4]: [Training] ce = 1.06835938 * 81920; err = 0.32983398 * 81920; totalSamplesSeen = 245760; learningRatePerSample = 0.003125; epochTime=0.284115s
01/12/2018 01:37:35: SGD: Saving checkpoint model 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.3'

01/12/2018 01:37:35: Starting Epoch 4: learning rate per sample = 0.003125  effective momentum = 0.810210  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 3: frames [245760..327680] (first utterance at frame 245760), data subset 0 of 1, with 1 datapasses

01/12/2018 01:37:35: Starting minibatch loop.
01/12/2018 01:37:35:  Epoch[ 4 of 4]-Minibatch[   1-  10, 6.25%]: ce = 1.03330250 * 5120; err = 0.32500000 * 5120; time = 0.0188s; samplesPerSecond = 272181.2
01/12/2018 01:37:35:  Epoch[ 4 of 4]-Minibatch[  11-  20, 12.50%]: ce = 1.03590166 * 4926; err = 0.31303289 * 4926; time = 0.0640s; samplesPerSecond = 77020.7
01/12/2018 01:37:35:  Epoch[ 4 of 4]-Minibatch[  21-  30, 18.75%]: ce = 1.01469421 * 5120; err = 0.32128906 * 5120; time = 0.0175s; samplesPerSecond = 293241.7
01/12/2018 01:37:35:  Epoch[ 4 of 4]-Minibatch[  31-  40, 25.00%]: ce = 1.01490192 * 5120; err = 0.31953125 * 5120; time = 0.0174s; samplesPerSecond = 293812.2
01/12/2018 01:37:35:  Epoch[ 4 of 4]-Minibatch[  41-  50, 31.25%]: ce = 0.99234657 * 5120; err = 0.30976562 * 5120; time = 0.0174s; samplesPerSecond = 293803.7
01/12/2018 01:37:36:  Epoch[ 4 of 4]-Minibatch[  51-  60, 37.50%]: ce = 0.99542236 * 5120; err = 0.30957031 * 5120; time = 0.0175s; samplesPerSecond = 293303.9
01/12/2018 01:37:36:  Epoch[ 4 of 4]-Minibatch[  61-  70, 43.75%]: ce = 0.99060974 * 5120; err = 0.30625000 * 5120; time = 0.0175s; samplesPerSecond = 292671.8
01/12/2018 01:37:36:  Epoch[ 4 of 4]-Minibatch[  71-  80, 50.00%]: ce = 1.01335754 * 5120; err = 0.31640625 * 5120; time = 0.0175s; samplesPerSecond = 293117.5
01/12/2018 01:37:36:  Epoch[ 4 of 4]-Minibatch[  81-  90, 56.25%]: ce = 0.99698181 * 5120; err = 0.31210938 * 5120; time = 0.0174s; samplesPerSecond = 293904.9
01/12/2018 01:37:36:  Epoch[ 4 of 4]-Minibatch[  91- 100, 62.50%]: ce = 0.97408829 * 5120; err = 0.31425781 * 5120; time = 0.0174s; samplesPerSecond = 293984.2
01/12/2018 01:37:36:  Epoch[ 4 of 4]-Minibatch[ 101- 110, 68.75%]: ce = 0.98822098 * 5120; err = 0.30429688 * 5120; time = 0.0216s; samplesPerSecond = 237387.6
01/12/2018 01:37:36:  Epoch[ 4 of 4]-Minibatch[ 111- 120, 75.00%]: ce = 1.00093079 * 5120; err = 0.30898437 * 5120; time = 0.0193s; samplesPerSecond = 264825.3
01/12/2018 01:37:36:  Epoch[ 4 of 4]-Minibatch[ 121- 130, 81.25%]: ce = 1.00725632 * 5120; err = 0.30976562 * 5120; time = 0.0181s; samplesPerSecond = 283084.1
01/12/2018 01:37:36:  Epoch[ 4 of 4]-Minibatch[ 131- 140, 87.50%]: ce = 0.97640991 * 5120; err = 0.30781250 * 5120; time = 0.0176s; samplesPerSecond = 290480.0
01/12/2018 01:37:36:  Epoch[ 4 of 4]-Minibatch[ 141- 150, 93.75%]: ce = 0.93567505 * 5120; err = 0.29589844 * 5120; time = 0.0175s; samplesPerSecond = 293344.2
01/12/2018 01:37:36:  Epoch[ 4 of 4]-Minibatch[ 151- 160, 100.00%]: ce = 0.97326965 * 5120; err = 0.30449219 * 5120; time = 0.0175s; samplesPerSecond = 293319.0
01/12/2018 01:37:36: Finished Epoch[ 4 of 4]: [Training] ce = 0.99624453 * 81920; err = 0.31113281 * 81920; totalSamplesSeen = 327680; learningRatePerSample = 0.003125; epochTime=0.33797s
01/12/2018 01:37:36: SGD: Saving checkpoint model 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech'

01/12/2018 01:37:36: Action "train" complete.


01/12/2018 01:37:36: ##############################################################################
01/12/2018 01:37:36: #                                                                            #
01/12/2018 01:37:36: # replaceCriterionNode command (edit action)                                 #
01/12/2018 01:37:36: #                                                                            #
01/12/2018 01:37:36: ##############################################################################


01/12/2018 01:37:36: Action "edit" complete.


01/12/2018 01:37:36: ##############################################################################
01/12/2018 01:37:36: #                                                                            #
01/12/2018 01:37:36: # sequenceTrain command (train action)                                       #
01/12/2018 01:37:36: #                                                                            #
01/12/2018 01:37:36: ##############################################################################

01/12/2018 01:37:36: 
Starting from checkpoint. Loading network from 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.0'.
NDLBuilder Using GPU 0
simplesenonehmm: reading 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/model.overalltying', 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list', 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/model.transprob'
simplesenonehmm: 83253 units with 45 unique HMMs, 132 tied states, and 45 trans matrices read
reading script file C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.scp ... 948 entries
trainlayer: OOV-exclusion code enabled, but no unigram specified to derive the word set from, so you won't get OOV exclusion
total 132 state names in state list C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/state.list
htkmlfreader: reading MLF file C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData/glob_0000.mlf ... total 948 entries
archive: opening 80 lattice-archive TOC files ('C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu\TestData\CY2SCH010061231_1369712653.numden.lats.toc' etc.).................................................................................. 923 total lattices referenced in 80 archive files
. [no lattice for An4/454/454/an70-meht-b]....... [no lattice for An4/89/89/an6-fjmd-b].. [no lattice for An4/683/683/an364-mmkw-b].. [no lattice for An4/476/476/an256-mewl-b].... [no lattice for An4/2/2/an253-fash-b]...............................................................................feature set 0: 250814 frames in 923 out of 948 utterances
minibatchutterancesource: out of 948 files, 0 files not found in label set and 25 have no lattice
label set 0: 129 classes
minibatchutterancesource: 923 utterances grouped into 3 chunks, av. chunk size: 307.7 utterances, 83604.7 frames
01/12/2018 01:37:36: 
Model has 29 nodes. Using GPU 0.

01/12/2018 01:37:36: Training criterion:   ce = SequenceWithSoftmax
01/12/2018 01:37:36: Evaluation criterion: err = ClassificationError

01/12/2018 01:37:36: Training 779396 parameters in 8 out of 8 parameter tensors and 21 nodes with gradient:

01/12/2018 01:37:36: 	Node 'HL1.W' (LearnableParameter operation) : [512 x 363]
01/12/2018 01:37:36: 	Node 'HL1.b' (LearnableParameter operation) : [512 x 1]
01/12/2018 01:37:36: 	Node 'HL2.W' (LearnableParameter operation) : [512 x 512]
01/12/2018 01:37:36: 	Node 'HL2.b' (LearnableParameter operation) : [512 x 1]
01/12/2018 01:37:36: 	Node 'HL3.W' (LearnableParameter operation) : [512 x 512]
01/12/2018 01:37:36: 	Node 'HL3.b' (LearnableParameter operation) : [512 x 1]
01/12/2018 01:37:36: 	Node 'OL.W' (LearnableParameter operation) : [132 x 512]
01/12/2018 01:37:36: 	Node 'OL.b' (LearnableParameter operation) : [132 x 1]

01/12/2018 01:37:36: No PreCompute nodes found, or all already computed. Skipping pre-computation step.
Setting Hsmoothing weight to 0.95 and frame-dropping threshhold to 1e-10
Setting SeqGammar-related parameters: amf=14.00, lmf=14.00, wp=0.00, bMMIFactor=0.00, usesMBR=false

01/12/2018 01:37:36: Starting Epoch 1: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 0: frames [0..81920] (first utterance at frame 0), data subset 0 of 1, with 1 datapasses
requiredata: determined feature kind as 33-dimensional 'USER' with frame shift 10.0 ms

01/12/2018 01:37:43: Starting minibatch loop.
dengamma value 1.007348
dengamma value 1.045203
dengamma value 1.059433
dengamma value 1.055064
dengamma value 1.040649
dengamma value 1.089017
dengamma value 0.966308
dengamma value 1.180848
dengamma value 1.011976
dengamma value 1.095109
dengamma value 1.020814
dengamma value 1.132270
dengamma value 1.033459
dengamma value 1.017764
dengamma value 1.089257
dengamma value 1.025332
dengamma value 1.093366
dengamma value 1.023830
dengamma value 1.042780
dengamma value 1.046108
dengamma value 0.988063
01/12/2018 01:37:46:  Epoch[ 1 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08511684 * 4628; err = 0.32778738 * 4628; time = 2.6090s; samplesPerSecond = 1773.9
dengamma value 1.086871
dengamma value 1.030537
dengamma value 1.099435
dengamma value 0.950085
dengamma value 1.117235
dengamma value 1.098321
dengamma value 1.051778
dengamma value 1.052108
dengamma value 0.981799
dengamma value 1.033564
dengamma value 0.997145
dengamma value 1.056693
dengamma value 1.031263
dengamma value 1.017093
dengamma value 1.121973
dengamma value 1.154042
dengamma value 1.124127
dengamma value 1.075708
dengamma value 1.096872
dengamma value 1.075896
dengamma value 1.077167
dengamma value 1.070469
01/12/2018 01:37:47:  Epoch[ 1 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.08330692 * 5946; err = 0.30575177 * 5946; time = 0.8979s; samplesPerSecond = 6621.9
dengamma value 1.031936
dengamma value 1.099014
dengamma value 1.090104
dengamma value 1.123960
dengamma value 1.051043
dengamma value 1.050564
dengamma value 1.088901
dengamma value 1.098076
dengamma value 1.016855
dengamma value 1.066787
dengamma value 1.004890
dengamma value 1.063285
dengamma value 1.072741
dengamma value 1.104201
dengamma value 1.047345
dengamma value 1.053868
dengamma value 1.030811
dengamma value 0.977386
dengamma value 1.072700
dengamma value 1.098058
dengamma value 1.073939
dengamma value 1.070296
01/12/2018 01:37:47:  Epoch[ 1 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.08576543 * 5916; err = 0.32302231 * 5916; time = 0.5867s; samplesPerSecond = 10084.0
dengamma value 1.022651
dengamma value 1.031886
dengamma value 1.028140
dengamma value 1.126292
dengamma value 1.095388
dengamma value 1.021869
dengamma value 1.059438
dengamma value 1.090211
dengamma value 1.096840
dengamma value 1.139418
dengamma value 1.035803
dengamma value 0.996777
dengamma value 1.094989
dengamma value 1.099669
dengamma value 0.944349
dengamma value 0.994805
dengamma value 1.065294
dengamma value 1.045118
dengamma value 1.042153
dengamma value 1.010778
dengamma value 1.082748
dengamma value 1.055556
01/12/2018 01:37:48:  Epoch[ 1 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.08213960 * 6386; err = 0.31788287 * 6386; time = 0.6265s; samplesPerSecond = 10192.4
dengamma value 1.036940
dengamma value 1.073367
dengamma value 1.068517
dengamma value 1.116013
dengamma value 1.066784
dengamma value 1.094507
dengamma value 1.074402
dengamma value 1.079125
dengamma value 1.156986
dengamma value 1.031709
dengamma value 1.071501
dengamma value 1.032944
dengamma value 1.086383
dengamma value 1.037973
dengamma value 1.093046
dengamma value 1.113993
dengamma value 1.045718
dengamma value 1.028972
dengamma value 1.043305
dengamma value 0.979336
dengamma value 1.094144
dengamma value 1.144139
dengamma value 1.038337
01/12/2018 01:37:49:  Epoch[ 1 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.08300250 * 6734; err = 0.29135729 * 6734; time = 0.6773s; samplesPerSecond = 9943.0
dengamma value 1.075078
dengamma value 1.113907
dengamma value 1.062569
dengamma value 1.082050
dengamma value 1.019022
dengamma value 0.975361
dengamma value 1.049451
dengamma value 1.125895
dengamma value 1.079664
dengamma value 1.050091
dengamma value 1.081015
dengamma value 1.024390
dengamma value 1.070326
dengamma value 0.971201
dengamma value 1.091554
dengamma value 1.042997
dengamma value 1.023944
dengamma value 1.105123
dengamma value 1.054380
dengamma value 1.064049
dengamma value 1.104168
dengamma value 1.045882
dengamma value 1.071176
dengamma value 0.969919
01/12/2018 01:37:49:  Epoch[ 1 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08671185 * 6202; err = 0.30990003 * 6202; time = 0.6178s; samplesPerSecond = 10039.7
dengamma value 1.125027
dengamma value 1.100356
dengamma value 1.052055
dengamma value 1.054449
dengamma value 1.132431
dengamma value 1.015649
dengamma value 1.102306
dengamma value 1.083894
dengamma value 1.037425
dengamma value 1.096277
dengamma value 1.097129
dengamma value 1.072034
dengamma value 1.100906
dengamma value 0.996911
dengamma value 0.946040
dengamma value 1.119299
dengamma value 1.081490
dengamma value 1.012345
dengamma value 1.057714
dengamma value 1.038277
dengamma value 1.058562
dengamma value 1.080738
dengamma value 1.046241
dengamma value 1.006915
01/12/2018 01:37:50:  Epoch[ 1 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.08143567 * 6362; err = 0.31452373 * 6362; time = 0.6511s; samplesPerSecond = 9770.5
dengamma value 1.063612
dengamma value 1.072375
dengamma value 1.107288
dengamma value 1.020348
dengamma value 1.054671
dengamma value 1.087350
dengamma value 1.013976
dengamma value 1.020642
dengamma value 1.011196
dengamma value 1.119753
dengamma value 1.105265
dengamma value 1.039464
dengamma value 1.055694
dengamma value 1.048525
dengamma value 1.050350
dengamma value 1.082347
dengamma value 1.085524
dengamma value 1.123222
dengamma value 1.028854
dengamma value 1.084959
dengamma value 1.058169
01/12/2018 01:37:51:  Epoch[ 1 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.07717225 * 5608; err = 0.31455064 * 5608; time = 0.5499s; samplesPerSecond = 10197.9
dengamma value 1.059225
dengamma value 1.034311
dengamma value 1.054466
dengamma value 1.000173
dengamma value 1.069283
dengamma value 1.024321
dengamma value 1.094703
dengamma value 0.980502
dengamma value 1.106294
dengamma value 1.091683
dengamma value 1.035409
dengamma value 1.106662
dengamma value 1.101379
dengamma value 1.033587
dengamma value 1.054450
dengamma value 1.010131
dengamma value 1.046065
dengamma value 1.020045
dengamma value 1.085864
dengamma value 1.075829
dengamma value 1.097336
dengamma value 1.039056
dengamma value 1.006671
01/12/2018 01:37:51:  Epoch[ 1 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08304269 * 6594; err = 0.32984531 * 6594; time = 0.7007s; samplesPerSecond = 9410.6
dengamma value 1.014027
dengamma value 0.873976
dengamma value 1.125899
dengamma value 0.999659
dengamma value 1.096841
dengamma value 1.034500
dengamma value 1.089791
dengamma value 1.095144
dengamma value 1.104581
dengamma value 1.097082
dengamma value 1.065593
dengamma value 1.054771
dengamma value 1.079037
dengamma value 1.074162
dengamma value 1.133456
dengamma value 1.026099
dengamma value 1.108134
dengamma value 1.128294
dengamma value 1.043710
dengamma value 1.054552
dengamma value 1.066906
dengamma value 0.997533
dengamma value 1.109002
01/12/2018 01:37:52:  Epoch[ 1 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08131694 * 6364; err = 0.31301069 * 6364; time = 0.6931s; samplesPerSecond = 9181.9
dengamma value 1.057347
dengamma value 1.025787
dengamma value 0.985562
dengamma value 1.054993
dengamma value 1.039285
dengamma value 1.106474
dengamma value 1.128026
dengamma value 1.066653
dengamma value 1.044803
dengamma value 1.140240
dengamma value 1.059612
dengamma value 1.142588
dengamma value 1.085782
dengamma value 1.001959
dengamma value 1.114644
dengamma value 1.050474
dengamma value 1.135521
dengamma value 1.063113
dengamma value 1.125537
dengamma value 1.139198
dengamma value 1.068946
dengamma value 1.044901
dengamma value 1.035377
dengamma value 1.104667
dengamma value 1.034499
dengamma value 0.995243
dengamma value 1.001171
01/12/2018 01:37:53:  Epoch[ 1 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08742834 * 6536; err = 0.30936353 * 6536; time = 0.6580s; samplesPerSecond = 9933.9
dengamma value 1.072333
dengamma value 1.053783
dengamma value 1.043247
dengamma value 1.062427
dengamma value 1.083152
dengamma value 1.035687
dengamma value 1.073474
dengamma value 1.062905
dengamma value 1.080180
dengamma value 1.012666
dengamma value 1.072081
dengamma value 1.093032
dengamma value 1.028948
dengamma value 0.927251
dengamma value 1.094961
dengamma value 1.044495
dengamma value 1.054462
dengamma value 1.009289
dengamma value 1.037297
dengamma value 1.018036
dengamma value 1.024852
01/12/2018 01:37:53:  Epoch[ 1 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08404769 * 6208; err = 0.31346649 * 6208; time = 0.6593s; samplesPerSecond = 9416.1
dengamma value 1.074217
dengamma value 1.064936
dengamma value 1.094712
dengamma value 1.061005
dengamma value 0.939648
dengamma value 1.039158
dengamma value 1.068864
dengamma value 1.087965
dengamma value 1.077173
dengamma value 1.039911
dengamma value 1.054315
dengamma value 1.121352
dengamma value 1.016612
dengamma value 1.174256
dengamma value 1.019680
dengamma value 1.079531
dengamma value 1.084686
dengamma value 1.076247
dengamma value 1.066118
dengamma value 1.075169
dengamma value 1.122456
dengamma value 1.113432
01/12/2018 01:37:54:  Epoch[ 1 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08231900 * 6326; err = 0.29465697 * 6326; time = 0.6679s; samplesPerSecond = 9471.6
dengamma value 1.074595
dengamma value 1.059277
dengamma value 1.095468
dengamma value 1.134148
dengamma value 0.987036
dengamma value 1.014474
dengamma value 1.097131
01/12/2018 01:37:54: Finished Epoch[ 1 of 3]: [Training] ce = 0.08299018 * 81936; err = 0.31073033 * 81936; totalSamplesSeen = 81936; learningRatePerSample = 2e-06; epochTime=17.805s
01/12/2018 01:37:54: SGD: Saving checkpoint model 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.1'

01/12/2018 01:37:54: Starting Epoch 2: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 1: frames [81920..163840] (first utterance at frame 81936), data subset 0 of 1, with 1 datapasses

01/12/2018 01:37:54: Starting minibatch loop.
dengamma value 1.142339
dengamma value 1.058890
dengamma value 1.127987
dengamma value 1.042544
dengamma value 1.100374
dengamma value 1.043773
dengamma value 1.082588
dengamma value 1.133881
dengamma value 1.038578
dengamma value 1.020133
dengamma value 1.011175
dengamma value 1.080624
dengamma value 1.071836
dengamma value 1.089253
dengamma value 1.024064
dengamma value 1.008864
dengamma value 1.046889
dengamma value 1.100840
dengamma value 1.040599
dengamma value 1.063464
dengamma value 1.076787
dengamma value 1.131891
dengamma value 1.024788
dengamma value 1.034280
01/12/2018 01:37:55:  Epoch[ 2 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08482508 * 6182; err = 0.29666775 * 6182; time = 0.7123s; samplesPerSecond = 8678.9
dengamma value 1.040652
dengamma value 0.992351
dengamma value 1.254414
dengamma value 1.030931
dengamma value 1.079942
dengamma value 1.110786
dengamma value 1.086542
dengamma value 1.028333
dengamma value 1.037839
dengamma value 1.073139
dengamma value 1.033413
dengamma value 1.109054
dengamma value 1.127404
dengamma value 1.023577
dengamma value 1.030338
dengamma value 1.077969
dengamma value 1.013053
dengamma value 1.109295
dengamma value 1.034388
dengamma value 1.022777
dengamma value 1.042674
dengamma value 1.103427
01/12/2018 01:37:56:  Epoch[ 2 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.08555367 * 5736; err = 0.28504184 * 5736; time = 0.6199s; samplesPerSecond = 9253.1
dengamma value 1.006573
dengamma value 1.090086
dengamma value 1.076999
dengamma value 1.078084
dengamma value 1.045815
dengamma value 1.150996
dengamma value 1.016243
dengamma value 1.125533
dengamma value 1.044597
dengamma value 1.041268
dengamma value 1.032035
dengamma value 1.046386
dengamma value 1.097585
dengamma value 1.154781
dengamma value 1.055330
dengamma value 1.161002
dengamma value 1.098976
dengamma value 1.064106
dengamma value 1.066841
dengamma value 1.089273
dengamma value 0.960769
dengamma value 0.967062
dengamma value 1.045449
01/12/2018 01:37:56:  Epoch[ 2 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.08194298 * 6244; err = 0.30253043 * 6244; time = 0.6506s; samplesPerSecond = 9597.4
dengamma value 1.075620
dengamma value 1.103513
dengamma value 1.068867
dengamma value 1.047586
dengamma value 1.134114
dengamma value 1.070781
dengamma value 1.061317
dengamma value 1.042682
dengamma value 1.058792
dengamma value 1.007387
dengamma value 1.083596
dengamma value 1.119744
dengamma value 1.110550
dengamma value 1.112888
dengamma value 1.004285
dengamma value 1.126172
dengamma value 1.015777
dengamma value 1.013878
dengamma value 1.065491
dengamma value 1.079948
dengamma value 1.021174
dengamma value 1.059560
dengamma value 1.147539
dengamma value 1.066184
dengamma value 1.037865
01/12/2018 01:37:57:  Epoch[ 2 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.08681048 * 6280; err = 0.30461783 * 6280; time = 0.6243s; samplesPerSecond = 10059.9
dengamma value 1.004589
dengamma value 1.071261
dengamma value 1.065129
dengamma value 1.073044
dengamma value 1.055937
dengamma value 1.063350
dengamma value 1.062459
dengamma value 1.048284
dengamma value 1.103089
dengamma value 1.059989
dengamma value 1.053591
dengamma value 1.032683
dengamma value 1.106049
dengamma value 1.049211
dengamma value 1.094461
dengamma value 1.086579
dengamma value 1.133182
dengamma value 1.080245
dengamma value 1.088999
dengamma value 1.052080
dengamma value 1.094907
dengamma value 1.074136
dengamma value 1.084447
dengamma value 1.079371
dengamma value 1.081831
dengamma value 1.095172
01/12/2018 01:37:58:  Epoch[ 2 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.07879096 * 7428; err = 0.29631125 * 7428; time = 0.7982s; samplesPerSecond = 9305.9
dengamma value 1.122269
dengamma value 1.078755
dengamma value 1.036547
dengamma value 1.129098
dengamma value 1.042817
dengamma value 1.030596
dengamma value 1.022431
dengamma value 1.079699
dengamma value 1.052139
dengamma value 1.057167
dengamma value 1.027897
dengamma value 1.028812
dengamma value 1.009410
dengamma value 1.117430
dengamma value 1.084801
dengamma value 1.072374
dengamma value 1.078960
dengamma value 0.968401
dengamma value 1.047249
dengamma value 1.067203
dengamma value 1.102195
dengamma value 1.010210
dengamma value 1.052377
01/12/2018 01:37:58:  Epoch[ 2 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08202231 * 6994; err = 0.31984558 * 6994; time = 0.6787s; samplesPerSecond = 10304.8
dengamma value 1.112913
dengamma value 1.070860
dengamma value 1.159802
dengamma value 1.013421
dengamma value 1.048120
dengamma value 1.107769
dengamma value 0.966331
dengamma value 1.126251
dengamma value 1.150292
dengamma value 1.058757
dengamma value 1.139743
dengamma value 1.105084
dengamma value 1.049913
dengamma value 1.023111
dengamma value 1.070847
dengamma value 1.054571
dengamma value 1.044098
dengamma value 1.031128
dengamma value 1.080383
dengamma value 1.079941
dengamma value 1.057421
dengamma value 1.140564
dengamma value 1.067853
dengamma value 1.068659
01/12/2018 01:37:59:  Epoch[ 2 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.07795315 * 6572; err = 0.30401704 * 6572; time = 0.6970s; samplesPerSecond = 9428.4
dengamma value 1.081276
dengamma value 1.002396
dengamma value 1.075706
dengamma value 1.079453
dengamma value 1.052944
dengamma value 1.111203
dengamma value 1.174176
dengamma value 1.057793
dengamma value 1.063149
dengamma value 1.050639
dengamma value 0.998082
dengamma value 1.036975
dengamma value 1.013639
dengamma value 1.135035
dengamma value 1.072257
dengamma value 0.933306
dengamma value 0.995550
dengamma value 1.030257
dengamma value 1.066744
dengamma value 1.077006
dengamma value 1.183285
dengamma value 1.006573
01/12/2018 01:38:00:  Epoch[ 2 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.08913368 * 5506; err = 0.32219397 * 5506; time = 0.5579s; samplesPerSecond = 9869.3
dengamma value 0.989019
dengamma value 1.066674
dengamma value 1.005540
dengamma value 1.062650
dengamma value 1.109487
dengamma value 1.120095
dengamma value 1.120539
dengamma value 1.070768
dengamma value 1.032565
dengamma value 1.007651
dengamma value 1.081200
dengamma value 0.948579
dengamma value 1.065760
dengamma value 1.022227
dengamma value 1.063691
dengamma value 1.114297
dengamma value 1.111893
dengamma value 1.067013
dengamma value 1.093322
dengamma value 0.995410
dengamma value 0.984840
01/12/2018 01:38:00:  Epoch[ 2 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08653753 * 5628; err = 0.33102345 * 5628; time = 0.5755s; samplesPerSecond = 9779.8
dengamma value 1.024676
dengamma value 1.106586
dengamma value 1.062672
dengamma value 1.081205
dengamma value 1.073164
dengamma value 0.972416
dengamma value 1.034082
dengamma value 0.998027
dengamma value 1.095735
dengamma value 1.050987
dengamma value 1.154428
dengamma value 1.080118
dengamma value 1.058833
dengamma value 1.061032
dengamma value 1.013459
dengamma value 1.034936
dengamma value 1.026584
dengamma value 1.048915
dengamma value 1.049611
dengamma value 1.116334
dengamma value 1.009040
dengamma value 1.023761
dengamma value 1.043757
dengamma value 0.981295
01/12/2018 01:38:01:  Epoch[ 2 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08331639 * 6032; err = 0.32360743 * 6032; time = 0.5926s; samplesPerSecond = 10178.6
dengamma value 1.027069
dengamma value 1.044925
dengamma value 1.058630
dengamma value 1.080989
dengamma value 1.058819
dengamma value 1.106030
dengamma value 1.073939
dengamma value 0.956326
dengamma value 1.120727
dengamma value 1.116844
dengamma value 1.026358
dengamma value 0.960776
dengamma value 1.050875
dengamma value 1.130319
dengamma value 1.077280
dengamma value 0.986301
dengamma value 1.129244
dengamma value 1.040561
dengamma value 1.093672
dengamma value 1.015843
01/12/2018 01:38:01:  Epoch[ 2 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08958534 * 4790; err = 0.32839248 * 4790; time = 0.5021s; samplesPerSecond = 9540.0
dengamma value 1.008231
dengamma value 0.955587
dengamma value 1.050479
dengamma value 1.056684
dengamma value 1.070668
dengamma value 1.002910
dengamma value 1.001383
dengamma value 1.086014
dengamma value 1.062474
dengamma value 0.951066
dengamma value 1.113687
dengamma value 1.047283
dengamma value 1.067949
dengamma value 1.011207
dengamma value 1.127242
dengamma value 1.059172
dengamma value 1.102998
dengamma value 1.149896
dengamma value 0.949410
dengamma value 1.135801
dengamma value 1.028012
dengamma value 1.035153
01/12/2018 01:38:02:  Epoch[ 2 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08638426 * 5986; err = 0.34012696 * 5986; time = 0.5864s; samplesPerSecond = 10208.5
dengamma value 1.020640
dengamma value 1.001421
dengamma value 1.041235
dengamma value 1.082228
dengamma value 1.021117
dengamma value 1.182353
dengamma value 1.096967
dengamma value 1.099794
dengamma value 1.061656
dengamma value 1.067380
dengamma value 1.085437
dengamma value 1.076160
dengamma value 1.000049
dengamma value 0.988415
dengamma value 1.022243
dengamma value 1.068997
dengamma value 1.042282
dengamma value 1.038365
dengamma value 1.109642
dengamma value 1.034115
dengamma value 1.097165
dengamma value 1.042833
dengamma value 1.080670
dengamma value 0.988018
01/12/2018 01:38:03:  Epoch[ 2 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08712232 * 7022; err = 0.31387069 * 7022; time = 0.6794s; samplesPerSecond = 10335.1
dengamma value 1.022061
dengamma value 1.090145
dengamma value 1.169946
dengamma value 1.064011
dengamma value 1.036161
dengamma value 1.064583
dengamma value 1.002640
dengamma value 1.064589
dengamma value 1.064589
01/12/2018 01:38:03: Finished Epoch[ 2 of 3]: [Training] ce = 0.08432031 * 82462; err = 0.31243482 * 82462; totalSamplesSeen = 164398; learningRatePerSample = 2e-06; epochTime=8.49077s
01/12/2018 01:38:03: SGD: Saving checkpoint model 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence.2'

01/12/2018 01:38:03: Starting Epoch 3: learning rate per sample = 0.000002  effective momentum = 0.995898  momentum as time constant = 2432.7 samples
minibatchiterator: epoch 2: frames [163840..245760] (first utterance at frame 164102), data subset 0 of 1, with 1 datapasses

01/12/2018 01:38:03: Starting minibatch loop.
dengamma value 1.083194
dengamma value 1.096060
dengamma value 1.072700
dengamma value 1.096994
dengamma value 1.046369
dengamma value 1.052031
dengamma value 1.137615
dengamma value 1.023755
dengamma value 1.102902
dengamma value 0.981247
dengamma value 1.101943
dengamma value 1.064149
dengamma value 1.058475
dengamma value 1.094174
dengamma value 1.057433
dengamma value 1.054381
dengamma value 1.094551
dengamma value 1.008675
dengamma value 1.083240
dengamma value 1.086545
dengamma value 1.038815
dengamma value 1.047767
dengamma value 1.032443
dengamma value 1.025625
01/12/2018 01:38:03:  Epoch[ 3 of 3]-Minibatch[   1-  10, 0.12%]: ce = 0.08416840 * 6292; err = 0.29497775 * 6292; time = 0.6285s; samplesPerSecond = 10010.4
dengamma value 1.153539
dengamma value 1.022342
dengamma value 0.982550
dengamma value 1.087099
dengamma value 1.072639
dengamma value 1.015715
dengamma value 1.040771
dengamma value 1.115719
dengamma value 1.027983
dengamma value 0.974966
dengamma value 0.944785
dengamma value 1.093239
dengamma value 1.052438
dengamma value 1.052508
dengamma value 1.026166
dengamma value 1.056617
dengamma value 1.011835
dengamma value 1.064380
dengamma value 1.079406
dengamma value 1.027075
dengamma value 1.053983
dengamma value 1.024170
01/12/2018 01:38:04:  Epoch[ 3 of 3]-Minibatch[  11-  20, 0.24%]: ce = 0.08039856 * 6596; err = 0.32747119 * 6596; time = 0.6698s; samplesPerSecond = 9848.3
dengamma value 1.069195
dengamma value 0.995794
dengamma value 1.087223
dengamma value 1.107086
dengamma value 1.058317
dengamma value 1.142983
dengamma value 1.021758
dengamma value 1.075444
dengamma value 1.043854
dengamma value 1.057575
dengamma value 1.095255
dengamma value 1.050555
dengamma value 1.015654
dengamma value 1.021272
dengamma value 1.113743
dengamma value 0.946499
dengamma value 1.045427
dengamma value 1.027281
dengamma value 1.017004
dengamma value 1.162265
dengamma value 1.068008
dengamma value 1.181165
01/12/2018 01:38:05:  Epoch[ 3 of 3]-Minibatch[  21-  30, 0.37%]: ce = 0.08691921 * 5666; err = 0.29809389 * 5666; time = 0.5936s; samplesPerSecond = 9544.6
dengamma value 1.112497
dengamma value 1.017814
dengamma value 1.053233
dengamma value 1.082148
dengamma value 1.068368
dengamma value 1.070024
dengamma value 1.099436
dengamma value 1.066434
dengamma value 1.084378
dengamma value 1.067402
dengamma value 1.084108
dengamma value 1.036388
dengamma value 0.990951
dengamma value 1.127245
dengamma value 1.101086
dengamma value 1.030670
dengamma value 1.080579
dengamma value 1.031822
dengamma value 1.086230
dengamma value 1.024286
dengamma value 1.077031
dengamma value 1.019533
01/12/2018 01:38:05:  Epoch[ 3 of 3]-Minibatch[  31-  40, 0.49%]: ce = 0.08187184 * 6626; err = 0.27890130 * 6626; time = 0.6985s; samplesPerSecond = 9486.4
dengamma value 1.081979
dengamma value 1.027456
dengamma value 1.023168
dengamma value 1.080344
dengamma value 1.112582
dengamma value 1.130227
dengamma value 1.071020
dengamma value 1.084242
dengamma value 1.014059
dengamma value 1.076116
dengamma value 1.135714
dengamma value 1.069694
dengamma value 1.037117
dengamma value 1.058439
dengamma value 1.019756
dengamma value 1.054875
dengamma value 1.080841
dengamma value 1.053030
dengamma value 1.089459
dengamma value 1.095453
dengamma value 1.053322
dengamma value 1.067419
dengamma value 1.048731
dengamma value 1.049758
01/12/2018 01:38:06:  Epoch[ 3 of 3]-Minibatch[  41-  50, 0.61%]: ce = 0.08252748 * 5652; err = 0.30095541 * 5652; time = 0.6203s; samplesPerSecond = 9111.7
dengamma value 1.084414
dengamma value 1.031529
dengamma value 0.992162
dengamma value 1.043801
dengamma value 1.024906
dengamma value 1.144334
dengamma value 1.033485
dengamma value 1.033445
dengamma value 1.101017
dengamma value 1.022309
dengamma value 1.017572
dengamma value 1.045706
dengamma value 1.070864
dengamma value 1.060947
dengamma value 1.078070
dengamma value 1.029016
dengamma value 1.000620
dengamma value 1.032773
dengamma value 1.095122
dengamma value 1.029699
dengamma value 1.048201
01/12/2018 01:38:07:  Epoch[ 3 of 3]-Minibatch[  51-  60, 0.73%]: ce = 0.08711125 * 6588; err = 0.33910140 * 6588; time = 0.6703s; samplesPerSecond = 9828.5
dengamma value 1.062161
dengamma value 1.088984
dengamma value 1.028247
dengamma value 0.980229
dengamma value 1.139203
dengamma value 1.058306
dengamma value 1.066094
dengamma value 1.032196
dengamma value 1.000268
dengamma value 1.078086
dengamma value 1.079221
dengamma value 1.022153
dengamma value 1.029286
dengamma value 1.110850
dengamma value 1.132698
dengamma value 1.046065
dengamma value 1.038020
dengamma value 1.070546
dengamma value 1.088454
dengamma value 1.054171
dengamma value 1.020130
01/12/2018 01:38:07:  Epoch[ 3 of 3]-Minibatch[  61-  70, 0.85%]: ce = 0.08696707 * 6328; err = 0.30009482 * 6328; time = 0.6758s; samplesPerSecond = 9363.6
dengamma value 1.056466
dengamma value 1.090677
dengamma value 0.999034
dengamma value 1.092621
dengamma value 1.111825
dengamma value 1.058586
dengamma value 1.075636
dengamma value 1.104576
dengamma value 1.043251
dengamma value 0.983013
dengamma value 1.042593
dengamma value 1.026183
dengamma value 1.014734
dengamma value 1.086755
dengamma value 1.042989
dengamma value 1.071160
dengamma value 1.066320
dengamma value 1.178620
dengamma value 1.086150
dengamma value 1.053086
dengamma value 1.053812
dengamma value 1.104475
dengamma value 1.024466
dengamma value 1.092009
dengamma value 1.067768
01/12/2018 01:38:08:  Epoch[ 3 of 3]-Minibatch[  71-  80, 0.98%]: ce = 0.08341956 * 6980; err = 0.28868195 * 6980; time = 0.7166s; samplesPerSecond = 9740.0
dengamma value 1.046095
dengamma value 0.965064
dengamma value 0.946434
dengamma value 1.106293
dengamma value 1.115870
dengamma value 1.019738
dengamma value 1.246401
dengamma value 1.098226
dengamma value 1.061911
dengamma value 1.105172
dengamma value 0.972315
dengamma value 1.065302
dengamma value 1.049215
dengamma value 1.005595
dengamma value 1.116473
dengamma value 1.052794
dengamma value 0.995687
dengamma value 1.252499
dengamma value 1.184100
dengamma value 1.040592
dengamma value 1.117776
dengamma value 0.973553
dengamma value 1.017957
01/12/2018 01:38:09:  Epoch[ 3 of 3]-Minibatch[  81-  90, 1.10%]: ce = 0.08081876 * 6774; err = 0.30897549 * 6774; time = 0.6310s; samplesPerSecond = 10735.4
dengamma value 0.976403
dengamma value 1.072767
dengamma value 1.106961
dengamma value 1.014770
dengamma value 0.929622
dengamma value 1.066092
dengamma value 1.036843
dengamma value 1.047531
dengamma value 0.932124
dengamma value 1.065244
dengamma value 1.007811
dengamma value 1.071722
dengamma value 1.016769
dengamma value 0.972719
dengamma value 1.062595
dengamma value 0.923402
dengamma value 1.042988
dengamma value 1.005806
dengamma value 1.062993
dengamma value 1.062059
dengamma value 1.088460
dengamma value 1.079139
01/12/2018 01:38:09:  Epoch[ 3 of 3]-Minibatch[  91- 100, 1.22%]: ce = 0.08960724 * 6146; err = 0.34933290 * 6146; time = 0.6460s; samplesPerSecond = 9514.4
dengamma value 1.012018
dengamma value 1.105966
dengamma value 1.052569
dengamma value 1.130140
dengamma value 0.949346
dengamma value 1.074186
dengamma value 1.060485
dengamma value 1.124159
dengamma value 1.087323
dengamma value 1.073173
dengamma value 1.033230
dengamma value 1.005165
dengamma value 1.165332
dengamma value 1.053087
dengamma value 1.104729
dengamma value 1.054737
dengamma value 1.077292
dengamma value 1.097218
dengamma value 1.005003
dengamma value 1.136119
dengamma value 1.053452
dengamma value 1.076506
01/12/2018 01:38:10:  Epoch[ 3 of 3]-Minibatch[ 101- 110, 1.34%]: ce = 0.08367936 * 5366; err = 0.30711890 * 5366; time = 0.5842s; samplesPerSecond = 9185.7
dengamma value 1.053830
dengamma value 1.043497
dengamma value 1.079359
dengamma value 1.056406
dengamma value 1.049954
dengamma value 1.002810
dengamma value 1.116078
dengamma value 1.059897
dengamma value 1.014654
dengamma value 1.022068
dengamma value 1.013809
dengamma value 1.058108
dengamma value 1.051142
dengamma value 1.029598
dengamma value 1.085361
dengamma value 1.076694
dengamma value 1.093749
dengamma value 1.093581
dengamma value 1.052751
dengamma value 0.977470
dengamma value 1.021709
dengamma value 1.105088
dengamma value 1.141075
dengamma value 1.083870
dengamma value 1.075425
01/12/2018 01:38:11:  Epoch[ 3 of 3]-Minibatch[ 111- 120, 1.46%]: ce = 0.08167383 * 6220; err = 0.31993569 * 6220; time = 0.6219s; samplesPerSecond = 10002.2
dengamma value 1.070086
dengamma value 1.044747
dengamma value 1.102025
dengamma value 1.072460
dengamma value 1.026249
dengamma value 1.058297
dengamma value 1.062631
dengamma value 1.099689
dengamma value 1.073688
dengamma value 1.079071
dengamma value 1.038607
dengamma value 1.054579
dengamma value 1.025938
dengamma value 1.088858
dengamma value 1.139057
dengamma value 1.107556
dengamma value 1.041547
dengamma value 0.990361
dengamma value 1.081367
dengamma value 1.026519
dengamma value 1.069204
dengamma value 1.084941
dengamma value 1.128955
01/12/2018 01:38:11:  Epoch[ 3 of 3]-Minibatch[ 121- 130, 1.59%]: ce = 0.08514163 * 6564; err = 0.29951249 * 6564; time = 0.6582s; samplesPerSecond = 9972.4
01/12/2018 01:38:11: Finished Epoch[ 3 of 3]: [Training] ce = 0.08413275 * 81798; err = 0.30863835 * 81798; totalSamplesSeen = 246196; learningRatePerSample = 2e-06; epochTime=8.41886s
01/12/2018 01:38:11: SGD: Saving checkpoint model 'C:\local\cygwin-2.8.2-x64\tmp\cntk-test-20180112013727.253967\Speech\DNN_SequenceTraining@release_gpu/models/cntkSpeech.sequence'

01/12/2018 01:38:12: Action "train" complete.

01/12/2018 01:38:12: __COMPLETED__
